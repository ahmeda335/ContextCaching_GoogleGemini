{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6ogYHKDBj5KGlgWdF+P59",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmeda335/ContextCaching_GoogleGemini/blob/main/ContextCaching_GoogleGemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Context Caching*** ðŸ“‘ðŸ’¾\n",
        "## Is a tool Google put in Gemini API to enable us extract information from some text stored in a cache for a certain time without the need of inserting this text in my prompt each time I want to get information from.<br>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Notes:<br>\n",
        "### ðŸ”” Make sure that you don't put any personal information, or passwords in the code in the unpaid sessions, as Google uses it to improve its AI performance.\n",
        "### ðŸ”” For paid models you can find pricing information here ---> https://ai.google.dev/pricing.\n",
        "### ðŸ”” In context cashing, pricing depends on how large the caching file, TTL (Time To Live) in the cache, and other factors such as non-cached input tokens and output tokens."
      ],
      "metadata": {
        "id": "9SMBg9f5jnIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ””ðŸ””ðŸ””***Fill in The cells between the two lines to create your model and then go below there is another cell between two lines to put your questons there*** ðŸ™‚"
      ],
      "metadata": {
        "id": "QuTumeaDEFFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JkiNDOrmEcie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here enter the name of the subject of the file, e.g. 'Book for learning Programming in Python'\n",
        "subject_title = \"Your Subject title\"\n",
        "\n",
        "# Time until the data will be removed from the cache, you can ask the model within this time. It is in minutes.\n",
        "cache_time = 20\n",
        "\n",
        "# The path of the file you want to cache.\n",
        "# Note: the file must be not small such that it has over than 32768 tokens. To calculate it, there is cell under in the code which explain more.\n",
        "file_path = 'Your file path, e.g. \"file.txt\"'  # You can use a text_fiel or a video_file also."
      ],
      "metadata": {
        "id": "W3aCJpfJBX_r"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Your Gemini-API-Key***<br>\n",
        "âœ… In the case of you are using colab, you can set your API-KEY in this way in the cell. If You are using any other development environment or cURL in the terminal it is preferred to put the API-KEY in an environment variable as shown in the next cell.<br>\n",
        "âœ… As I am using colab I will use the first method.<br><br>\n",
        "You can get your Gemini API Key from here -->  https://aistudio.google.com/app/apikey"
      ],
      "metadata": {
        "id": "7AhVbVEik_CY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‡ðŸ‘‡Put Your API-KEY here.<br>\n",
        "\n",
        "![Untitled.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAE0AAAF1CAYAAACtV2fEAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAABHbSURBVHhe7Z15lBTVFYeJS0ABURGQZUDUEBWPSgaGfRgY2R1OZDdBYECHbYBhk92ICLKDyOLI6oiAbLLJpuZEgYiQEA4DIvsiILtsJjH/vPTvmdep6b49XXW7mqGm7x/fGai69frVN6/qvXpTt1+B4iNeVoIzRBqDAjVq1FCCM0QaA5HGQKQxEGkMRBoDkcZApDEQaQxEGgORxkCkMRBpDEQaA5HGQKQxEGkMRBoDkcZApDG4JdKS6iaods2eVd1bP6W6tqys6tVJIOPsgGNbNnlOveIr5w8vPKMa169KxkWTqElrUj9ezegbpw7Mvlf9vLxAEMfmFFKbxzyoxnevGFZiK5+k9/uXU/tnFibLuvThXWrNGyVUxktPkMe7TVSkvfnKo+qi70SoE6S4mHWXmpURp5LrVctRTqOkqmrxsIfVPz/+FXkcxT+mF1FdXnw6Rzlu46q0RF+LQeuhTsYOVxffqRr973J7tVVldcEnk4oLx799LHytjKpVs3pQHd3ANWkNaldTn4/mCwMbRhfXZU3pVUGfOBXjhA3DS6ik2vz7ZyhckVbTx6YuJdWxfkXU8fcLkScQjmuL71DNk+PV4JcrkfudcvTde9TRPkXVxtRSun5Uvbm4Im1G20fUuX73ao72L6pOzS1Inkhu4J7Wvvmz6uZS+/evUByefo863Luov06Z7SqQ9eYSsbQW9X6nzmQU9lcQHPKJO2kRt+7Nh9Twzr9RbX3DjgaJ1VRa68pqUs9H1JkFv9b7T84rqOr6LqOvJtzvP4bihK/MCT0e0fe7+nWr6WHMyNTH1edj/39bOOJrYQfT78tRH9C+wXNk/TlELG35y2WCKggODyii9s8orAZ2+C15HEBvCaGDOlTSYziroECWjyylx3tUOQC/lL1TiqiDvYOFAdSTOo5DRNKSaiWoH4gKArQ+J7/djW8VJ2WBpcNLkccE0j75uaBWb61P3Vru9KYRSevT9CmygmBK60fJY0JxeRE9vDjuGwRjKEMdQzGtTUWyPqBX08rkMU6JSNrkVo+SlUPrS/S1QuoYiqYN4klhAMMP6phQYOhD1QlM9NWXOsYpEUmb/1IcWblvuj1IxoeiQ8ozpDDAGd3v9H0+Va+57cuT8U6JSNrKjnQnsOiP5cj4UKS3e5IUhgFuHcZ9CJ9P1WuFr75UvFMikra4Q1mycus6P0zGhwIzFpQ08Hw957MYazo9TNYL9aXinRKRtFltK5CV29erGBkfCkz1UMJAn/bOZy6yexYj64VBOBXvlIikvfbCE2TlQM8m9nsqPObgMYqShoErdUwoevg+l6oPGNjcnamjiKQl1wndU6G1ORkXff526If9vjbnyfB5+32fS9UHPTp6Vuo4p0QkDXydVpysJMjsFqdqhxGHR6tmvgf1cd0rksLAjreLqZZJVcjjDbVrVg/ZAYCtrz5EHschYmkjWlQiK7ltxAP6hPfNLKxaNw1+MsAlOTW9grqx5A61+k8lVMOkqnoGNlDY8VmF9LPkqb6F1VsvPk7OWLSqX0XtSKOHGYahKaEf55wSsTScxF8DWtvXr+d88MbQ4VDmPWrD6IfUh0NKqx1T7gt6Auj8+6eDWtvxmYXUgfScl9vR3kXVli4l9cwFnif/1v2BHPsp3GxlIGJpAL/pE32K6ApuH/5LC3PKnneLqFo1fb+AycX0/0/NLhgkjAPqFe7Sdoor0kC67zn0q6GRzdyO6vqYnvnYPbGoTxg9W+EE3PzRm1L1jQTXpIFebZ9UPyy8mxRih/NZd+v5tuaJ8SEfhexyOqOw6tfsSbKekeKqNIAbOuePK+gQ8Gc689hUv3aC2pBaihQSDgw7OjV8JqhubuG6NMOI1MfVlUV3koKs/GtZAfXJn0rqmQ6qnA7PP6tWdSytWw4lyApu+EN8vSSGH1RZbhE1aQBT2Km+XnFsWkW1bEQp9Y2v19w+qZha+XpJNbNvnJ6qpoYjFJhqyvBdbrN9vebHvmdI/CHnk06l9UwLhiK4pKnjokFUpeVXRBoDkcZApDEQaQxEGgORxkCkMRBpDEQaA5HGQKQxEGkMRBoDkcZApDEQaQxEGgORxkCkMRBpDEQaA5HGQKQxEGkMCqSmpirBGZ5qaagwtf1WI9IYiDQGIo2BSGMg0hiINAYijYFIYyDSGIg0BiKNgUhjINIYiDQLnTp1UiNHjtQ/qf0GkWZhxYoV6qefftI/qf0GkWZBpDEQaQxEWhjS0tLUwYMH1aVLl/xcuXJFS8NP63bEId4cK9IsckQag6hdnhMmTFAbNmxQM2fOVPXr1ydjooUnpS1cuFBdvXpVF3zz5k315Zdf6u1t2rRRWVlZavXq1Y6YO3du0Gfkhiel7dmzRxdqOHnypOrRo4cG/7bus8O3335Lfk4oPClt27ZtOU768OHD+pGDuqnaYffu3eTnhCLa0qLyGNWnTx/13Xff6Uvz+++/V2PHjiXjokW0pdnFcUeQl4g0Bp6UlpKSotasWaPOnj2rdu3apbp160bGRQtPSlu3bp2+n5mOADdyjNVw49y6davKzs52xObNm8nPCYUnpWGIYO0989uQwy7S0hjIPY2BI2l5jUhj4Elp8kTwC46k5fdnT7s4kpbfZzns4kja7TCfdjvguCPI65nbKlWqRI0yZcrYwrG0vATS4uPjo0bZsmVt4TlpVatWjRrlypWzheekVatWzU9CQoKrxMXF2cJz0qwnWb16dVehBFF4Uhp1wm5Qvnx5W3hOmjlBan+kUIIoPCmN2ucGlCAKkWaBEkThOWnUdregBFGINAuUIAqRZoESRCHSLFCCKESaBUoQhUizQAmiEGkWKEEUIs0CJYhCpFmgBFGINAuUIAqRZoESRCHSLFCCKESaBUoQhUizQAmiEGkWKEEUIs0CJYhCpFmgBFGINAuUIAqRZoESRCHSLFCCKESaBUoQhUizQAmiiEhau3btVGZmptqxY4d+xX3v3r3q008/VQMHDiTjI8XT0vBe2vLly9Xly5eD3m40L/zhrUm3X5n3rDQIwxuQJgkDP8+dO6f279+v38G9fv26X96JEyf0y81UORw8Kw2vfd64cUNLgaRRo0bl2I9LFlJNDGRimzWGiyelDRs2TLcqyDh06JDq0qULGQeMXLB06VIyximelLZy5Up9Of7444/qnXfeIWMMaF14TR6C8RMvM1NxTvCktO3bt2sJR48eDZvnDdavX6/jz5w5o/r160fGOMGT0kzqot33/823DyDRAgn1VIwTPCnNJF8g1cfO6/BG2oULF/T9kIpxgielffbZZ1rC+fPnbUnYuXOnjrd7OYfDk9KmTp2qv7wInQGSYnNrbegoTKzTZNhQeFIaJCEJDK0HQ4m1a9eS4pDVYoYmp06dcm2A60lpAAIw0ocQtKLjx4/rbOMxY8aoOXPm6Pue9akA9zO0UKosp3hWGhgwYIAee0GakRMIekzzfWRuifO0NIDLEll0R44c8bcsXLKnT5/WXxCApwWIgjC3xHleml3cFBcz0oBb4mJKGnBDXMxJA5GKi0lpwIjDTMn8+fPJmFDErDQwfvx4NXnyZHJfbsS0NC4ijYFIYyDSGIg0BiKNQbSl2UWkMRBpDEQaA5HGQKQxEGkMRBoDkcZApDEQaQxEGgORxkCkMRBpDEQaA5HGQKQxEGkMRBoDkcZApDEQaQwcS0tOTlaDBw/W6xE0aNCAjAlFUlKSmjVrlj6+ZcuWZExueFLakCFD9CtSeAEZmXWJiYlkXCjq1KmjkzaQwYd31N577z0yLhSek9a4cWN/JnGkKTtYkwXlgCZNmpAxFJ6T1rVrV/+JQiAVY5eGDRv6y3KS/uM5ab179/afKE46cD/uUdOmTdPrryxYsEDVqlVLb2/UqJH+P7bj39iG1+lNWT179sxRTm7kK2nIYkEuwYEDB/wx2Na6dWt/dgtAB4B4keYDPSpE1K1bV2eqIAZZyEhxRAubMWOGWrVqlT+PKialQRIVA6yrmCHpn+phsc3E5Gtp+IISnCRaUs2aNckYgGGEEdKxY0cyBpgWmZGRQe6n8JQ0SNq0aZM+yY8++oiMMaSnp/ul4Z5GxQCTdYw8Kmo/hWekITHMZAgjdzPcUwByA4y0N954g4wBuMSRfIY4fO1O586dyTgrnm1pSIylYkCvXr10Jp5ZTBBfrUPFgS+++ELHbNy4MdfL3Ypn72kXL14k97do0UIvyztx4kS1aNEiHYshCPZh4cEpU6b4YzGGw36A3FGzPRyek4ZezpwohhbYhnU9kXa9bNkyPbzYsmWL3j506FB/LJ5RcdOHVFNWvXr1/Pv79+/v3x4Oz0nDpWdOFLMV2IZ7ltmGNGwzDsPI3yxpCQYNGpSjLOs4LV+3NEoabuaTJk1S06dPD3rwRi86e/ZsckgR09K4xIw0DAnMieb2RGAHHG/Kwn2RiqHwnDQ8b167dk2faKSVhyiUg8lMM/NhB89JA0hwxcliaDFu3Dg9E0vFhQK97pIlS3SPi47C6WSmJ6UBPLjjBg9Mb2kX8zcCTJtzLnHPSstLRBoDkcZApDEQaQxEGgORxkCkMRBpDEQaA5HGQKQxEGkMRBoDkcZApDEQaQxEGgORxkCkMYiWtNGvP6b+vu0+/ZPaH0jMS5s7s5xS1wtofr50h0pMTCDjrMS0NKswsGVtcTIukJiVNn50xRzCvv7z/bZaGYhJac2bxetLkSMMxKS0VUtK+YUd2lPYkTAQc9IC72Ov9a9ExuVGTEkLFIbLkooLR8xIo4Q5vSwNMSHNTWHAE9Iy0p/QI/a5s5292gXcFgZue2kQduOHu/QJ/+fynWRMKKIhDNz20i6duDvHiVMxFNESBtjS8NY2lp8EbiwOGArriQMqJpBoCgOOpKWkpOgl3MziMgasVHbs2DGdWUwdxwUjd+vJAyrOSrSFAdvSsJwu1lnPbSk3ZBcjKdbpa6WhWPpB6RwCdvylIhlnuBXCgC1pkGBWW4Q0rIqNlRWxHTKR44kldrEfCWXIKqbKcULg8yFISwudP3qrhAFb0ubNm6dfh4cwZA7jMg2Msa7GiBaHfCi7YDHCwHwC6/MhwLQNNU57PrlqUIuMpjBgS5pZdhcpifhODWzDNyAg/3Pfvn3+tdixECq+7ASxTrCWCwJbDYCEQGmY3jHDEUO0hYGw0tACTAtCK8M2s0qsOWkku6KlYdnwrKwsvQa7HXbt2hUkLXCey4B9VmmU2FshDISVhpMxSa9IocY2k0ptQCKF06/AAaYcI426jxkQD2nU5Xj2cEHb8/tucMtaGjoHkzyGdUGxLbClBcowmGnoD+bUzJPLMZBbdk+ztkaznrsB5b47tXmQEAOk5OXlGIgtaZH0nvhSJxxHSTNx6D2vnbk3SEoobvXlGIgtaZGM07APwihp+GnirFKyvymS4/9W8qp1WbElDXCfCIw0Kt4qLVQHEA0ibam2pQHOs6ddabOmlidPMFrgl2StpxMcSbNid5YDLRR5nSbWSmB+Oy679StKkCfpNnjisH62E9jS8gLudLfbiDQGIo2BSGMg0hiINAYijYFIYyDSGIg0BiKNgUhjINIYiDQGIo2BSGMg0hiINAYijYFIYyDSGIg0BiKNgUhjINIYiDQGIo2BSGMg0hiINAYijYFIY2BbGl4JxTrFW7duVdnZ2SThlnyLFM9Jw3v/eAmZeg3UgNdEc1uVLFI8JQ2tDItpGTHmdXcr5r1a/IRg6nXRQJwm13pKmjXVx2StBIKFt8xr8XZB1ktmZiZZHkW+kwY44pDxQpVFkS+lOQHloDxr6lA4RJpIc45IYyDSGIg0BiKNgUhjINIYiDQGIo1BvpdmZ5bDKWZWBOVa07dzw1PSgJ35NKegPJRLfR6F56TZmbl1AspBeXZbGfCctNsBkcZApDEQaQxEGgORxkCkMRBpDEQaA5HGQKQxEGkMRBoDkcZApDEQaQxEGgORxkCkMRBpDEQaA5HGQKQxEGkMRBoDkcZApDEQaQxEGgORxkCkMRBpDEQag9tCWrf5Is05DUQaB5HGQKQxEGkMRBoDkcZApDEQaQxEGgORxkCkMRBpDEQaA5HGQKQxEGkMRBoDkcZApDEQaQxEGgORxkCkMRBpDESaY2qo/wKnkr3dygsvTAAAAABJRU5ErkJggg==)"
      ],
      "metadata": {
        "id": "HMGMnU1YhCrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('GEMINI-API-KEY')"
      ],
      "metadata": {
        "id": "3HCsg_yPjhQj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… If you don't use google colab"
      ],
      "metadata": {
        "id": "4bhyC_tmn01z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()\n",
        "\n",
        "# !export GEMINI-API-KEY = <YOUR_API_KEY>   # Replace '<YOUR_API_KEY>' with you API-KEY. If you are using the terminal don't use '!' before 'export'.\n",
        "# API_KEY = os.getenv('GEMINI-API-KEY')"
      ],
      "metadata": {
        "id": "cLYsOPzDn3w1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "l8RjofxXEhXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… The Python SDK for the Gemini API to use Context Cashing is contained in the google-generativeai package."
      ],
      "metadata": {
        "id": "8lvby0adi8Yf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xzAp7tzdidvz"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Importing required libraries."
      ],
      "metadata": {
        "id": "gGr447Q6knpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.generativeai import caching\n",
        "import datetime\n",
        "import time"
      ],
      "metadata": {
        "id": "MiMFE02ykl58"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "R-W_76U_4xnk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Getting the text or video-file you want to cache.<br>\n",
        "âœ… Cached contents mustn't be less than 32768 tokens."
      ],
      "metadata": {
        "id": "G5QGhggvo9SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vQJqyb95V6S",
        "outputId": "4a376d20-19f4-4c29-d3d2-e6ed10bf0735"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_text_file = file_path"
      ],
      "metadata": {
        "id": "i_8rjlhqpDUY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Setting the model you will use."
      ],
      "metadata": {
        "id": "JbR0cX_BpvBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'models/gemini-1.5-flash-001'"
      ],
      "metadata": {
        "id": "aJgrisPTpt0f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Uploading the loaded file to the API"
      ],
      "metadata": {
        "id": "kxcXVGeSqEI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = genai.upload_file(path=path_to_text_file)"
      ],
      "metadata": {
        "id": "3x99vMGUp72n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Wait for the file to finish processing"
      ],
      "metadata": {
        "id": "SCMEREqwqn4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while text_file.state.name == 'PROCESSING':\n",
        "  print('Waiting for file to be processed.')\n",
        "  time.sleep(2)\n",
        "  text_file = genai.get_file(text_file.name)\n",
        "\n",
        "print(f'File processing complete: {text_file.uri}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "UA9fjU38qblF",
        "outputId": "cd72203a-cea1-4611-bc66-cfe5facd2c9a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for file to be processed.\n",
            "Waiting for file to be processed.\n",
            "Waiting for file to be processed.\n",
            "Waiting for file to be processed.\n",
            "File processing complete: https://generativelanguage.googleapis.com/v1beta/files/70odf8fhtmru\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… ***Calculating*** the ***number of tokens*** in the file you want to cache."
      ],
      "metadata": {
        "id": "_q87fhHOytoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Text ###\n",
        "# The number of tokens depends on the big of the text \"Note: The token is not a word, it maybe a word or a subword, e.g. playing is a word but two tokens 'play', and 'ing'. The process of making tokens is called Tokenization.\"\n",
        "\n",
        "### Image ###\n",
        "# The number of tokens for image is fixed = 258 tokens, despite of the size of the image.\n",
        "\n",
        "### Video ###\n",
        "# The number of tokens for video is fixed = 263/second.\n",
        "\n",
        "### Audio ###\n",
        "# The number of tokens for audio is fixed = 32/second.\n",
        "\n",
        "# For more information ---> https://ai.google.dev/gemini-api/docs/tokens?lang=python\n",
        "\n",
        "# ------------------------------------------------------- #\n",
        "\n",
        "# To get the number of tokens.\n",
        "model = genai.GenerativeModel(model_name)\n",
        "print(\"total_tokens: \", model.count_tokens(text_file))\n",
        "\n",
        "# There is another way, using 'response.usage_metadata' after creating the response, as shown later in the notebook."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "9j10h7FrzEL6",
        "outputId": "591c0bca-0477-4a6b-d633-beb4e3a362aa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_tokens:  total_tokens: 335120\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Here I initialize the ***Cache*** with ttl = 5 minutes.<br>\n",
        "âœ… TTL ---> (Time To Live) is the time that the loaded file will remain in the cache before it is deleted. The default value is 1 hour, and there is no limit above."
      ],
      "metadata": {
        "id": "vz_sYZnXr22S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache = caching.CachedContent.create(\n",
        "    model=model_name,\n",
        "    display_name= subject_title, # 'Small description of the text_file, e.g. \"Book to learn programming in Python\"', # used to identify the cache\n",
        "    system_instruction=(\n",
        "        'You are an expert video analyzer, and your job is to answer '\n",
        "        'the user\\'s query based on the file you have access to.'\n",
        "    ),\n",
        "    contents=[text_file],\n",
        "    ttl=datetime.timedelta(minutes=cache_time),\n",
        ")"
      ],
      "metadata": {
        "id": "55ckZMdOrUCp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a GenerativeModel which uses the created cache.\n",
        "model = genai.GenerativeModel.from_cached_content(cached_content=cache)"
      ],
      "metadata": {
        "id": "s2uBWPRjtmsw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5oGTop-4Fnql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Your Quesiton\" # Write your quextion here and then run the next cell to run the answer"
      ],
      "metadata": {
        "id": "KP-SHBR3FW20"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cGYHvJWRFoeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the model\n",
        "response = model.generate_content([question])\n",
        "\n",
        "# Printing the answer.\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "0vxWcxWut2Fp",
        "outputId": "2dde628e-f925-4a9b-f389-41d87c6ab699"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The initial functions are `pygame.init()`, `pygame.display.set_mode(SCREEN_WIDTH, SCREEN_HEIGHT)`, and `pygame.display.set_caption(\"Shooter\")`. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.usage_metadata) # Number of tokens for the prompt"
      ],
      "metadata": {
        "id": "k_xcviHrFNeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… It's not possible to retrieve or view cached content, but you can retrieve cache metadata (name, model, display_name, usage_metadata, create_time, update_time, and expire_time).\n",
        "\n"
      ],
      "metadata": {
        "id": "UgUpLCjSudFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for c in caching.CachedContent.list():\n",
        "  print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "4CQPocPNuw8U",
        "outputId": "9287236e-01a7-4d59-ed0b-a41aebe90216"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CachedContent(\n",
            "    name='cachedContents/mjbls3740ion',\n",
            "    model='models/gemini-1.5-flash-001',\n",
            "    display_name='Religious Question',\n",
            "    usage_metadata={\n",
            "        'total_token_count': 56373,\n",
            "    },\n",
            "    create_time=2024-08-15 07:43:39.696457+00:00,\n",
            "    update_time=2024-08-15 07:43:39.696457+00:00,\n",
            "    expire_time=2024-08-15 07:48:37.923305+00:00\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Update a Cache\n",
        "âœ… You can set a new ttl or expire_time for a cache. Changing anything else about the cache isn't supported."
      ],
      "metadata": {
        "id": "a_M05wL1vCzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cache.update(ttl=datetime.timedelta(minutes=10))"
      ],
      "metadata": {
        "id": "L3bkE9lVvCgy"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Delete a Cache"
      ],
      "metadata": {
        "id": "-gpaLKezvZBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cache.delete()"
      ],
      "metadata": {
        "id": "gooKN4-avbd2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fmrVqG2B_pNv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}